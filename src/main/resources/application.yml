server:
  # Port to run api from
  port: 8080

  forward-headers-strategy: native

  # HTTP redirect. Not necessary but I include it here anyway. Uncomment to use of course.
#  tomcat:
#    additional-connectors:
#      port: 8080
#      protocol: HTTP/1.1
#      redirect-to-https: true

cors:
  allowed-origins:
    - http://localhost
    - https://chat.coosanta.net # Just to make my own website easier to configure, you can remove this.
    - https://chat.coosanta.net/*
  allowed-methods:
    - GET
    - POST
    - PUT
    - DELETE
    - OPTIONS
  allowed-headers:
    - Content-Type
    - Authorization
    - Accept
    - X-Requested-With
    - Origin
  allow-credentials: false

spring:
  mvc:
    dispatch-options-request: true
  application:
    name: LLM-API

  # Uncomment either jks or pem depending on the certificate type. Don't do both!
  # TODO: Test https
#  ssl:
#    bundle:
#      jks:
#        mybundle:
#          key:
#            alias: "application"
#          keystore:
#            location: "classpath:application.p12"
#            password: "secret"
#            type: "PKCS12"
#      pem:
#        mybundle:
#          keystore:
#            certificate: "classpath:application.crt"
#            private-key: "classpath:application.key"

app:
  # File path where conversations are stored relative to project root.
  conversation-path: ./conversations/

  # Whether to save completion conversations
  # this is so you can access conversations sent in by completions in the conversation interface as well
  # Confused? So am I.
  # Potentially large and excess conversations in conversations so only use if necessary.
  save-completion-conversations: true

  # Maximum amount of conversations to store in memory.
  # Lower values may increase disk activity but reduce memory usage (small impact)
  loaded-conversations-limit: 100 # TODO: FIX!

  # Whether to ping the client regularly to prevent it from timing out. Especially useful
  # on slow hardware and/or large models and/or large context.
  sse-ping: true

  # Interval in seconds to send ping
  ping-interval: 10


  model:
    # Path to the gguf file relative to project root. Uses placeholder llama.gguf for now.
    path: ./models/llama.gguf

    # How much context the model can store in tokens.
    # Higher number will result in the model being able to
    # remember more of the conversation but will use more RAM.
    # Llama.cpp default 512 (model might be a bit forgetful and will struggle on long responses).
    # Set to 0 to load model default (might use too much ram)
    # Parallel sequences will equally divide context between each other.
    # For example, 1000 context shared among 4 parallel sequences will be 250 context per slot.
    context: 1024

    # How many prompt tokens one batch is. If the prompt is 8 tokens with a batch size of 4 it will
    # process as 2 batches of 4.
    batch-size: 512

    # How many token the model generates each response.
    # It is not recommended to have the number higher than context as this can result
    # in weird responses (endless repetition of the same token)
    # Set to -1 for infinity
    # Set to -2 for until context filled (for some reason this doesn't work for me)
    response-limit: 400

    # How many threads the model uses. The more threads the faster
    # the model responds (not always true, see https://github.com/ggerganov/llama.cpp/blob/master/docs/development/token_generation_performance_tips.md).
    # However, this can result in other applications being impacted due to less resources.
    threads: 4

    # How many concurrent completions the model can do.
    # If there are more than the configured number of parallel sequences it will queue
    # the prompt to be responded to once a spot frees up.
    # The more parallel sequences the model can do the slower it may become on each response
    # If there are multiple responses being generated at once.
    parallel-sequences: 2

    # Set the number of layers to store in VRAM (use default: -1)
    # * You need to compile this yourself if you want GPU offloading.
    # See https://github.com/kherud/java-llama.cpp?tab=readme-ov-file#setup-required
    gpu-layers: -1

    # Time in minutes after last activity before the model unloads from memory.
    # Set to 0 to load the model only when generating.
    # Set to -1 if you want the model to never unload (until program terminates).
    inactivity-timeout: 10

    # Whether to load the model on application startup or not.
    # If false, the model will only load once `/chat` is called from api.
    # Loading on startup will decrease the time the model responds on the
    # first `/chat` the application receives at the cost of more
    # background resource use (especially RAM).
    # Inactivity timeouts still apply, which means if the model doesn't do anything for
    # the configured time since start it will still unload the model.
    # If you want the model to stay loaded for the entirety of the application's
    # lifecycle, set this to true and `inactivity-timeout` to -1.
    load-on-start: false
