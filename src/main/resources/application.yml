server:
  # Port to run api from
  port: 8080

  forward-headers-strategy: native

  # HTTP redirect. Not necessary but I include it here (commented out) anyway.
#  tomcat:
#    additional-connectors:
#      port: 8080
#      protocol: HTTP/1.1
#      redirect-to-https: true

spring:
  application:
    name: LLM-API

  # Uncomment either jks or pem depending on the certificate type. Don't do both!
  # TODO: Test https
#  ssl:
#    bundle:
#      jks:
#        mybundle:
#          key:
#            alias: "application"
#          keystore:
#            location: "classpath:application.p12"
#            password: "secret"
#            type: "PKCS12"
#      pem:
#        mybundle:
#          keystore:
#            certificate: "classpath:application.crt"
#            private-key: "classpath:application.key"

app:
  # File path where conversations are stored relative to project root.
  conversation-path: ./conversations/

  # Maximum amount of conversations to store in memory.
  # Lower values may increase disk activity but reduce memory usage (small impact)
  loaded-conversations-limit: 100


  model:
    # Path to the gguf file relative to project root. Uses placeholder llama.gguf for now.
    path: ./models/llama.gguf

    # How much "memory" the model has.
    # Higher number will result in the model being able to
    # remember more of the conversation but will use more RAM.
    # Llama.cpp default 512 (model might be a bit forgetful).
    # Set to 0 to load model default (might use too much ram)
    context: 512

    # How many threads the model uses. The more threads the faster
    # the model responds. However, this can result in other
    # applications being impacted due to less resources.
    threads: 6

    # Set the number of layers to store in VRAM (use default: -1)
    gpu-layers: -1

    # Time in minutes after last activity before the model unloads from memory.
    # Set to 0 to load the model only when generating.
    # Set to -1 if you want the model to never unload (until program terminates).
    inactivity-timeout: 10

    # Whether to load the model on application startup or not.
    # If false, the model will only load once `/chat` is called from api.
    # Loading on startup will decrease the time the model responds on the
    # first `/chat` the application receives at the cost of more
    # background resource use (especially RAM).
    # Inactivity timeouts still apply, which means if the model doesn't do anything for
    # the configured time since start it will still unload the model.
    # If you want the model to stay loaded for the entirety of the application's
    # lifecycle, set this to true and `inactivity-timeout` to -1.
    load-on-start: false

    # TODO: Scrolling window thing

    # more setting to come!